{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "disaster_descriptions = pd.read_csv('data/disaster/disaster_descriptions.csv')\n",
    "disaster_points = pd.read_csv('data/disaster/disaster_points.csv')\n",
    "disaster_outlines = gpd.read_file('data/disaster/disaster_outlines/disaster_outlines.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_outlines = gpd.read_file('data/states/state_outlines.shp')\n",
    "county_outlines = gpd.read_file('data/counties/county_outlines.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_state = pd.read_csv('data/housing/mid/housing_state.csv')\n",
    "housing_us = pd.read_csv('data/housing/mid/housing_us.csv')\n",
    "housing_county = pd.read_csv('data/housing/mid/housing_county.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "disaster_to_state = pd.read_csv('data/disaster/disaster_to_state.csv')\n",
    "disaster_to_county = pd.read_csv('data/disaster/disaster_to_county.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# disaster_to_state = disaster_outlines.merge(disaster_points.loc[disaster_points['level'] == 1], on='geo_id', how='right').sjoin(state_outlines, how='left')[['disaster_id', 'state_id']]\n",
    "# disaster_to_state.to_csv('data/disaster/disaster_to_state.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "# housing_us = pd.read_csv('sources/housing/top/Metro_zhvi_uc_sfrcondo_tier_0.67_1.0_sm_sa_month.csv')\n",
    "# housing_us.loc[housing_us['RegionName'] == 'United States'].to_csv('data/housing/top/housing_us.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "# state_outlines = gpd.read_file('data/states/state_outlines.shp')\n",
    "# housing_state = pd.read_csv('sources/housing/top/State_zhvi_uc_sfrcondo_tier_0.67_1.0_sm_sa_month.csv')\n",
    "# state_code = state_outlines[['state_id', 'State']]\n",
    "# housing_state = housing_state.merge(state_code, left_on='RegionName', right_on='State', how='left')\n",
    "# housing_drop = ['RegionID', 'SizeRank', 'RegionType', 'StateName', 'State']\n",
    "# housing_state = housing_state.drop(columns=housing_drop)\n",
    "# df2 = housing_state.rename({'RegionName': 'State'}, axis='columns')\n",
    "# df2.to_csv('data/housing/top/housing_state.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "# housing_county = pd.read_csv('sources/housing/top/County_zhvi_uc_sfrcondo_tier_0.67_1.0_sm_sa_month.csv')\n",
    "# county_outlines = gpd.read_file('sources/tl_2022_us_county/tl_2022_us_county.shp')\n",
    "# state_outlines = gpd.read_file('sources/tl_2022_us_state/tl_2022_us_state.shp')\n",
    "# state_county = housing_county[['State', 'RegionName']]\n",
    "# state_code = state_outlines[['STATEFP', 'STUSPS']]\n",
    "# county_outlines = state_county.merge(county_outlines.merge(state_code, on='STATEFP', how='left'), left_on=['State', 'RegionName'], right_on=['STUSPS', 'NAMELSAD'], how='left')\n",
    "# county_columns = ['State', 'RegionName', 'GEOID', 'geometry']\n",
    "# county_outlines = county_outlines[county_columns]\n",
    "# county_outlines = county_outlines.rename({'GEOID': 'county_id', 'RegionName': 'County'}, axis='columns')\n",
    "\n",
    "# housing_county['county_id'] = county_outlines['county_id']\n",
    "# housing_drop = ['RegionID', 'SizeRank', 'RegionType', 'StateName',\n",
    "#         'Metro', 'StateCodeFIPS', 'MunicipalCodeFIPS']\n",
    "# housing_county = housing_county.drop(columns=housing_drop)\n",
    "# housing_county.to_csv('data/housing/top/housing_county.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "# county_outlines = gpd.read_file('sources/tl_2022_us_county/tl_2022_us_county.shp')\n",
    "# state_outlines = gpd.read_file('sources/tl_2022_us_state/tl_2022_us_state.shp')\n",
    "# state_code = state_outlines[['STATEFP', 'STUSPS']]\n",
    "# county_outlines = county_outlines.merge(state_code, on='STATEFP', how='left')\n",
    "# county_columns = ['STUSPS', 'NAMELSAD', 'GEOID', 'geometry']\n",
    "# county_outlines = county_outlines[county_columns]\n",
    "# county_outlines = county_outlines.rename({'GEOID': 'county_id', 'RegionName': 'County'}, axis='columns')\n",
    "# county_outlines['county_id'] = county_outlines['county_id'].astype('int64')\n",
    "# county_outlines.to_file('data/counties/county_outlines.shp', driver='ESRI Shapefile')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "# state_code = state_outlines[['GEOID', 'NAME']]\n",
    "# housing_state = housing_state.merge(state_code, left_on='RegionName', right_on='NAME', how='left')\n",
    "# housing_drop = ['RegionID', 'SizeRank', 'RegionType', 'StateName', 'NAME']\n",
    "# housing_state = housing_state.drop(columns=housing_drop)\n",
    "# df2 = housing_state.rename({'GEOID': 'state_id', 'RegionName': 'State'}, axis='columns')\n",
    "# df2.to_csv('data/housing/sfh/housing_state.csv', index=False)\n",
    "\n",
    "# states = housing_state['GEOID']\n",
    "# state_outlines = state_outlines.loc[state_outlines['GEOID'].isin(states)]\n",
    "# state_columns = ['GEOID', 'STUSPS', 'NAME', 'geometry']\n",
    "# state_outlines = state_outlines[state_columns]\n",
    "# df2 = state_outlines.rename({'GEOID': 'state_id', 'NAME': 'State'}, axis='columns')\n",
    "# df2.to_file('data/states/state_outlines.shp', driver='ESRI Shapefile')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "# disaster_to_county = disaster_outlines.merge(disaster_points.loc[disaster_points['level'] == 2], on='geo_id', how='right').sjoin(county_outlines, how='left')[['disaster_id', 'county_id']]\n",
    "# disaster_to_county.to_csv('data/disaster/disaster_to_county.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# disaster_dest_cols = ['Dis No', 'Disaster Type', 'Disaster Subtype', \n",
    "#        'Disaster Subsubtype', 'Event Name', 'Location', \n",
    "#        'Dis Mag Value', 'Dis Mag Scale', 'Start Year', 'Start Month', \n",
    "#        'Start Day', 'End Year', 'End Month', 'End Day', 'Total Deaths', \n",
    "#        'No Injured', 'No Affected', 'No Homeless', 'Total Affected', \n",
    "#        \"Total Damages ('000 US$)\", \"Total Damages, Adjusted ('000 US$)\"]\n",
    "\n",
    "# disaster_descriptions = disaster_descriptions[disaster_dest_cols]\n",
    "\n",
    "# disaster_descriptions['Dis No'] = disaster_descriptions['Dis No'].apply(lambda x: x.split('-')[0] + '-' + x.split('-')[1])\n",
    "# disaster_descriptions.loc[disaster_descriptions['End Month'].isna(), 'End Month'] = disaster_descriptions['Start Month']\n",
    "\n",
    "# disasters = disaster_points['disaster_id'].unique()\n",
    "# disaster_descriptions = disaster_descriptions.loc[disaster_descriptions['Dis No'].isin(disasters)]\n",
    "\n",
    "# startdf = disaster_descriptions[['Start Year', 'Start Month', 'Start Day']]\n",
    "# enddf = disaster_descriptions[['End Year', 'End Month', 'End Day']]\n",
    "\n",
    "# enddf.loc[enddf['End Day'].isna(), 'End Day'] = 28\n",
    "# startdf.loc[startdf['Start Day'].isna(), 'Start Day'] = 1\n",
    "\n",
    "# enddf.columns = ['year', 'month', 'day']\n",
    "# startdf.columns = ['year', 'month', 'day']\n",
    "\n",
    "# disaster_descriptions['start_date'] = pd.to_datetime(startdf)\n",
    "# disaster_descriptions['end_date'] = pd.to_datetime(enddf)\n",
    "\n",
    "# disaster_descriptions.drop(columns=['Start Year', 'Start Month', 'Start Day', 'End Year', 'End Month', 'End Day'], inplace=True)\n",
    "# df2 = disaster_descriptions.rename({'Dis No': 'disaster_id'}, axis='columns')\n",
    "\n",
    "# df2.to_csv('data/disaster/disaster_descriptions.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# disaster_outlines = gpd.read_file('sources/disaster/usa-disasters.gpkg')\n",
    "# outlines = disaster_outlines['geometry'].reset_index()\n",
    "# outlines = outlines.drop_duplicates(subset='geometry')\n",
    "# outlines = outlines.drop(columns=['index']).reset_index(drop=True).reset_index()\n",
    "# disaster_outlines = disaster_outlines.merge(outlines, on='geometry')\n",
    "\n",
    "# indices = disaster_outlines[['geo_id', 'index']]\n",
    "# disaster_points = disaster_points.merge(indices, on='geo_id')\n",
    "# outlines.to_file('data/disaster/disaster_outlines/disaster_outlines.shp', driver='ESRI Shapefile')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# disaster_points = pd.read_csv('sources/disaster/usa-points.csv')\n",
    "# disaster_point_cols = ['Unnamed: 0', 'geo_id', 'level',\n",
    "#        'geolocation', 'adm1', 'disastertype', 'disasterno', 'latitude',\n",
    "#        'longitude']\n",
    "\n",
    "# disaster_points = disaster_points[disaster_point_cols]\n",
    "# df2 = disaster_points.rename({'Unnamed: 0': 'point_id', 'disasterno': 'disaster_id', 'geolocation': 'location', 'adm1': 'state'}, axis='columns')\n",
    "# df2.to_csv('data/disaster/disaster_points.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "# housing_county = pd.read_csv('data/housing/sfh/housing_county.csv')\n",
    "# housing_county = housing_county.melt(['RegionName', 'State', 'county_id'], var_name='Date', value_name='Value')\n",
    "# housing_county.to_csv('data/housing/sfh/housing_county.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "# housing_us = pd.read_csv('data/housing/top/housing_us.csv')\n",
    "# housing_us = housing_us.drop(columns=['RegionID', 'SizeRank', 'RegionType', 'StateName'])\n",
    "# housing_us = housing_us.melt(['RegionName'], var_name='Date', value_name='Value')\n",
    "# housing_us.to_csv('data/housing/top/housing_us.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "# housing_state = pd.read_csv('data/housing/top/housing_state.csv')\n",
    "# housing_state = housing_state.melt(['State', 'state_id'], var_name='Date', value_name='Value')\n",
    "# housing_state.to_csv('data/housing/top/housing_state.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
